"""Grid search script for running experiments across models, scalers, and parameters."""

from __future__ import annotations

import argparse
import concurrent
import csv
import json
import multiprocessing
import os
import pickle
import shutil
import subprocess
import time
from concurrent.futures import ProcessPoolExecutor  # pyright: ignore
from dataclasses import asdict, dataclass
from pathlib import Path
from threading import Lock
from typing import Any

from loguru import logger as log
from sampleworks.utils.guidance_constants import GuidanceType, StructurePredictor
from sampleworks.utils.guidance_script_arguments import GuidanceConfig, JobConfig, JobResult


@dataclass
class GridSearchConfig:
    models: list[str]
    scalers: list[str]
    ensemble_sizes: list[int]
    gradient_weights: list[float]
    gd_steps: list[int]
    methods: list[str]
    proteins_file: str
    output_dir: str


def get_job_status(job: JobConfig) -> str:
    """
    Check the status of a job by inspecting its log file.

    Returns:
        'success': Job completed successfully (has "Final loss:" in log)
        'failed': Job ran but failed (has errors/traceback in log or exit != 0)
        'not_run': Job has not been executed yet (no log file)
    """
    if not os.path.exists(job.log_path):
        return "not_run"

    try:
        with open(job.log_path) as f:
            log_content = f.read()

        has_error = (
            "Traceback" in log_content or "AssertionError" in log_content or "Error:" in log_content
        )
        if has_error:
            return "failed"

        if "Final loss:" in log_content:
            return "success"

        return "failed"
    except Exception as e:
        log.warning(f"Error reading log file {job.log_path}: {e}")
        return "failed"


def detect_gpus() -> list[str]:
    cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES", "")
    if cuda_visible:
        return [g.strip() for g in cuda_visible.split(",") if g.strip()]
    try:
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=index", "--format=csv,noheader"],
            capture_output=True,
            text=True,
        )
        if result.returncode == 0:
            return [g.strip() for g in result.stdout.strip().split("\n") if g.strip()]
    except FileNotFoundError:
        pass
    return ["0"]


def get_pixi_env(model: str) -> str:
    if model in (StructurePredictor.BOLTZ_1, StructurePredictor.BOLTZ_2):
        return "boltz"
    elif model == StructurePredictor.PROTENIX:
        return "protenix"
    elif model == StructurePredictor.RF3:
        return "rf3"
    else:
        valid_options = [m.value for m in StructurePredictor]
        raise ValueError(f"Unknown model: {model}. Valid options are: {valid_options}")


def build_args_for_process_pool(
    job: JobConfig, args: argparse.Namespace, device_num: int | None = None
) -> GuidanceConfig:
    guidance_config = GuidanceConfig(
        protein=job.protein,
        structure=job.structure_path,
        density=job.density_path,
        model=job.model,
        guidance_type=job.scaler,
        log_path=job.log_path,
        output_dir=job.output_dir,
        loss_order=args.loss_order,
        partial_diffusion_step=args.partial_diffusion_step,
        resolution=job.resolution,
        device=f"cuda:{device_num}" if device_num is not None else "",
        gradient_normalization=args.gradient_normalization,
        augmentation=args.augmentation,
        align_to_input=args.align_to_input,
    )
    # given model_type and guidance_type, the GuidanceConfig class will set itself up
    # with defaults for remaining required args, but we want to set them further here.
    guidance_config.populate_config_for_guidance_type(job, args)
    return guidance_config


def run_grid_search(
    jobs: list[JobConfig],
    gpus: list[str],
    args: argparse.Namespace,
    job_statuses: dict[int, str] | None = None,
) -> list[JobResult]:
    """
    Replacing run_job and run_grid_search, avoiding model reloads
    of the model.
    Args:
        jobs: generated by generate_and_filter_jobs, a list of JobConfig objects
        gpus: the available GPUs to run jobs on, really we only use the length of the list
        args: command-line arguments to this script, used to pass some on to jobs.
        job_statuses: a dictionary mapping job IDs to their statuses,
            also generated by generate_and_filter_jobs.

    Returns:
        list of JobResult objects, one for each job run, primarily to track success/failure
    """
    results: list[JobResult] = []
    successful = 0
    failed = 0

    max_workers = len(gpus)
    log.info(f"Running {len(jobs)} jobs with {max_workers} parallel workers")

    # Divide the job among the workers:
    worker_job_queues = [
        [build_args_for_process_pool(j, args, i) for j in jobs[i::max_workers]]
        for i in range(max_workers)
    ]
    # we'll pickle each job queue separately and then execute each job queue in a separate process
    # in principle we could just pass the job queue directly to the worker_wrapper function, but
    # this keeps a record of what we did, which may be useful for debugging.
    job_queue_paths = []
    for wjq in worker_job_queues:
        wjq_path = os.path.join(args.output_dir, f"wjq_{id(wjq)}.pkl")
        log.info(f"Pickling worker job queue to {wjq_path}")
        job_queue_paths.append(wjq_path)
        with open(wjq_path, "wb") as f:
            pickle.dump(wjq, f)

    if args.dry_run:
        log.info(f"[DRY-RUN] Running {len(jobs)} jobs with {max_workers} parallel workers")
        log.info(f"[DRY-RUN] Job queue paths: {job_queue_paths}")
        return results

    # Clean up output directories if they already exist and have failed previously:
    for i, job in enumerate(jobs):
        clean_output = False
        if job_statuses is not None:
            clean_output = job_statuses.get(id(job), "not_run") != "not_run"
        if clean_output and os.path.exists(job.output_dir):
            log.info(f"Cleaning existing output directory: {job.output_dir}")
            shutil.rmtree(job.output_dir)

    # TODO this approach works, but I think it may be more efficient to actually call a
    #  script that runs the jobs (a script that basically only calls run_guidance_job_queue
    #  to avoid pickling objects.
    with ProcessPoolExecutor(
        max_workers=max_workers,  # TODO: may need to tune this or make a flag.
        mp_context=multiprocessing.get_context("spawn"),
    ) as executor:
        futures = {}

        for worker_num, job_queue_path in enumerate(job_queue_paths):
            model = worker_job_queues[worker_num][0].model
            future = executor.submit(
                run_guidance_queue_script, (job_queue_path, max_workers, model, worker_num)
            )
            futures[future] = job_queue_path

        for completed in concurrent.futures.as_completed(futures):  # pyright: ignore
            try:
                with open(futures[completed].replace(".pkl", ".results.pkl"), "rb") as f:
                    result = pickle.load(f)

                results.extend(result)
                for r in result:
                    if r.status == "success":
                        successful += 1
                        log.info(
                            f"SUCCESS ({r.protein}, {r.model}, {r.method}, {r.scaler} "
                            f"{r.runtime_seconds:.1f}s): {r.log_path}"
                        )
                    else:
                        failed += 1
                        log.error(
                            f"FAILED ({r.protein}, {r.model}, {r.method}, {r.scaler} "
                            f"exit={r.exit_code}): {r.log_path}"
                        )
            except Exception as e:
                failed += 1
                log.error(f"Job failed with exception: {e}")  # this won't be very informative
    return results


def run_guidance_queue_script(args: tuple[str, int, str, int]):
    job_queue_path, max_workers, model, worker_num = args
    pixi_env = get_pixi_env(model)
    script_path = Path(__file__).parent / "scripts" / "run_guidance_pipeline.py"
    cmd = f"pixi run -e {pixi_env} python {script_path} --job-queue-path {job_queue_path}"
    cmd = cmd.split()
    log.info(f"Running worker {worker_num}: {cmd} on GPU {worker_num % max_workers}")
    # env = os.environ.copy()

    with open(job_queue_path.replace(".pkl", ".log"), "w") as log_file:
        result = subprocess.run(cmd, stdout=log_file, stderr=subprocess.STDOUT)
    return result


def main(args: argparse.Namespace):
    """
    Main pipeline for running grid search experiments.
    Args:
        args: Command-line arguments.
    """
    gpus = detect_gpus()
    log.info(f"Detected {len(gpus)} GPUs: {gpus}")
    if args.max_parallel != "auto":
        gpus = gpus[: int(args.max_parallel)]

    log_args(args, gpus)

    if len(args.models.split()) > 1:
        # this is designed to run one type of model per script, # TODO to allow multiple models
        raise ValueError("Multiple --models selected, this is not compatible with the new script!")
    if len(args.methods.split(",")) > 1:
        # this is designed to run one type of model per script, # TODO to allow multiple models
        raise ValueError("Multiple --methods selected, this is not compatible with the new script!")

    filtered_jobs, job_statuses = generate_and_filter_jobs(args)

    if len(filtered_jobs) == 0:
        log.info("No jobs to run!")
        return

    config = GridSearchConfig(
        models=args.models.split(),
        scalers=args.scalers.split(),
        ensemble_sizes=[int(x) for x in args.ensemble_sizes.split()],
        gradient_weights=[float(x) for x in args.gradient_weights.split()],
        gd_steps=[int(x) for x in args.num_gd_steps.split()],
        methods=[m.strip() for m in args.methods.split(",")],
        proteins_file=args.proteins,
        output_dir=args.output_dir,
    )

    start_time = time.time()

    results = run_grid_search(filtered_jobs, gpus, args, job_statuses=job_statuses)

    if not args.dry_run and results:
        save_results(results, config, args.output_dir, time.time() - start_time)

    log.info("=" * 50)
    log.info("Grid search complete")
    log.info("=" * 50)


def generate_jobs(args: argparse.Namespace) -> list[JobConfig]:
    jobs = []

    with open(args.proteins, newline="") as f:
        reader = csv.DictReader(f)
        proteins = list(reader)

    models = args.models.split()
    scalers = args.scalers.split()
    ensemble_sizes = [int(x) for x in args.ensemble_sizes.split()]
    gradient_weights = [float(x) for x in args.gradient_weights.split()]
    gd_steps_list = [int(x) for x in args.num_gd_steps.split()]
    methods = [m.strip() for m in args.methods.split(",")]

    for protein in proteins:
        structure = protein["structure"].strip()
        density = protein["density"].strip()
        resolution = float(protein["resolution"].strip())
        protein_name = protein["name"].strip()

        for model in models:
            model_methods = methods if model == StructurePredictor.BOLTZ_2 else [None]

            for method in model_methods:
                method_suffix = f"_{method.replace(' ', '_')}" if method else ""

                for scaler in scalers:
                    if scaler == GuidanceType.FK_STEERING:
                        for ens in ensemble_sizes:
                            for gw in gradient_weights:
                                for gd in gd_steps_list:
                                    output_dir = os.path.join(
                                        args.output_dir,
                                        protein_name,
                                        f"{model}{method_suffix}",
                                        scaler,
                                        f"ens{ens}_gw{gw}_gd{gd}",
                                    )
                                    log_path = os.path.join(output_dir, "run.log")
                                    jobs.append(
                                        JobConfig(
                                            protein=protein_name,
                                            structure_path=structure,
                                            density_path=density,
                                            resolution=resolution,
                                            model=model,
                                            scaler=scaler,
                                            ensemble_size=ens,
                                            gradient_weight=gw,
                                            gd_steps=gd,
                                            method=method,
                                            output_dir=output_dir,
                                            log_path=log_path,
                                        )
                                    )
                    else:
                        for ens in ensemble_sizes:
                            for gw in gradient_weights:
                                output_dir = os.path.join(
                                    args.output_dir,
                                    protein_name,
                                    f"{model}{method_suffix}",
                                    scaler,
                                    f"ens{ens}_gw{gw}",
                                )
                                log_path = os.path.join(output_dir, "run.log")
                                jobs.append(
                                    JobConfig(
                                        protein=protein_name,
                                        structure_path=structure,
                                        density_path=density,
                                        resolution=resolution,
                                        model=model,
                                        scaler=scaler,
                                        ensemble_size=ens,
                                        gradient_weight=gw,
                                        gd_steps=1,
                                        method=method,
                                        output_dir=output_dir,
                                        log_path=log_path,
                                    )
                                )

    return jobs


print_lock = Lock()


def save_results(
    results: list[JobResult],
    config: GridSearchConfig,
    output_dir: str,
    total_time: float,
):
    os.makedirs(output_dir, exist_ok=True)
    results_path = os.path.join(output_dir, "results.json")

    existing_runs = []
    if os.path.exists(results_path):
        try:
            with open(results_path) as f:
                existing_data = json.load(f)
                existing_runs = existing_data.get("runs", [])
            log.info(f"Loaded {len(existing_runs)} existing results")
        except Exception as e:
            log.warning(f"Could not load existing results: {e}")

    new_run_keys = {
        (
            r.protein,
            r.model,
            r.method,
            r.scaler,
            r.ensemble_size,
            r.gradient_weight,
            r.gd_steps,
        )
        for r in results
    }

    merged_runs = [asdict(r) for r in results]
    for existing_run in existing_runs:
        key = (
            existing_run.get("protein"),
            existing_run.get("model"),
            existing_run.get("method"),
            existing_run.get("scaler"),
            existing_run.get("ensemble_size"),
            existing_run.get("gradient_weight"),
            existing_run.get("gd_steps"),
        )
        if key not in new_run_keys:
            merged_runs.append(existing_run)

    output = {
        "config": asdict(config),
        "runs": merged_runs,
        "summary": {
            "total": len(merged_runs),
            "successful": sum(1 for r in merged_runs if r.get("status") == "success"),
            "failed": sum(1 for r in merged_runs if r.get("status") == "failed"),
            "total_runtime_seconds": round(total_time, 2),
        },
    }

    with open(results_path, "w") as f:
        json.dump(output, f, indent=2)

    log.info(f"Results saved to {results_path} ({len(merged_runs)} total runs)")


def parse_args() -> argparse.Namespace:
    """
    Parse command-line arguments for the grid search script.
    
    The returned namespace contains configuration for dataset, model selection, grid parameters,
    checkpoints, FK-steering options, diffusion/guidance settings, parallelism, and run-control flags.
    Key attributes include:
    - proteins, models, scalers, ensemble_sizes, gradient_weights, num_gd_steps, output_dir
    - boltz1_checkpoint, boltz2_checkpoint, protenix_checkpoint, rf3_checkpoint, methods
    - num_particles, fk_lambda, fk_resampling_interval
    - partial_diffusion_step, loss_order, use_tweedie, gradient_normalization, augmentation, align_to_input
    - max_parallel, dry_run, force_all, only_failed, only_missing
    
    Returns:
        argparse.Namespace: Parsed command-line arguments.
    """
    parser = argparse.ArgumentParser(
        description="Run grid search across models, scalers, and parameters."
    )

    parser.add_argument(
        "--proteins",
        required=True,
        help="CSV file with columns: structure,density,resolution,name",
    )

    parser.add_argument("--models", default="boltz2 protenix", help="Space-separated models")
    parser.add_argument(
        "--scalers", default="pure_guidance fk_steering", help="Space-separated scalers"
    )
    parser.add_argument(
        "--ensemble-sizes", default="1 2 4 8", help="Space-separated ensemble sizes"
    )
    parser.add_argument(
        "--gradient-weights",
        default="0.01 0.1 0.2",
        help="Space-separated gradient weights",
    )
    parser.add_argument(
        "--num-gd-steps",
        default="20",
        help="Space-separated GD steps (FK steering only)",
    )
    parser.add_argument("--output-dir", default="./grid_search_results", help="Output directory")

    parser.add_argument(
        "--boltz1-checkpoint",
        default=os.environ.get("BOLTZ1_CHECKPOINT", "/checkpoints/boltz1_conf.ckpt"),
        help="Boltz1 checkpoint path (default: $BOLTZ1_CHECKPOINT or"
        " /checkpoints/boltz1_conf.ckpt)",
    )
    parser.add_argument(
        "--boltz2-checkpoint",
        default=os.environ.get("BOLTZ2_CHECKPOINT", "/checkpoints/boltz2_conf.ckpt"),
        help="Boltz2 checkpoint path (default: $BOLTZ2_CHECKPOINT or "
        "/checkpoints/boltz2_conf.ckpt)",
    )
    parser.add_argument(
        "--protenix-checkpoint",
        default=os.environ.get("PROTENIX_CHECKPOINT", ""),
        help="Protenix checkpoint path",
    )
    parser.add_argument(
        "--rf3-checkpoint",
        default=os.environ.get("RF3_CHECKPOINT", ""),
        help="RF3 checkpoint path",
    )
    parser.add_argument(
        "--methods",
        default="X-RAY DIFFRACTION",
        help="Comma-separated methods for Boltz2",
    )

    parser.add_argument("--num-particles", type=int, default=3, help="FK steering: num particles")
    parser.add_argument("--fk-lambda", type=float, default=0.5, help="FK steering: lambda")
    parser.add_argument(
        "--fk-resampling-interval",
        type=int,
        default=1,
        help="FK steering: resampling interval",
    )

    parser.add_argument(
        "--partial-diffusion-step", type=int, default=0, help="Partial diffusion step"
    )
    parser.add_argument("--loss-order", type=int, default=2, help="L1 (1) or L2 (2) loss")
    parser.add_argument("--use-tweedie", action="store_true", help="Use Tweedie (pure guidance)")
    parser.add_argument(
        "--gradient-normalization",
        action="store_true",
        help="Enable gradient normalization",
    )
    parser.add_argument("--augmentation", action="store_true", help="Enable augmentation")
    parser.add_argument("--align-to-input", action="store_true", help="Align to input structure")

    parser.add_argument(
        "--max-parallel",
        default="auto",
        help="Max parallel jobs (default: auto = number of GPUs)",
    )
    parser.add_argument("--dry-run", action="store_true", help="Print commands without executing")

    parser.add_argument(
        "--force-all",
        action="store_true",
        help="Re-run all jobs, including successful ones (overrides default)",
    )
    parser.add_argument(
        "--only-failed",
        action="store_true",
        help="Run only failed jobs, skip un-run and successful jobs",
    )
    parser.add_argument(
        "--only-missing",
        action="store_true",
        help="Run only un-run jobs, skip failed and successful jobs",
    )

    return parser.parse_args()


def log_args(args: argparse.Namespace, gpus: list[str]):
    log.info("=" * 50)
    log.info("Starting grid search")
    log.info(f"Models: {args.models}")
    log.info(f"Scalers: {args.scalers}")
    log.info(f"Ensemble sizes: {args.ensemble_sizes}")
    log.info(f"Gradient weights: {args.gradient_weights}")
    log.info(f"GD steps: {args.num_gd_steps}")
    log.info(f"Boltz2 methods: {args.methods}")
    log.info(f"Output directory: {args.output_dir}")
    log.info(f"GPUs: {gpus}")
    log.info(f"Dry run: {args.dry_run}")
    log.info("=" * 50)


# TODO make job statuses a proper class
# TODO: there are many constants here like "not_run" that should be defined in only one place.
def generate_and_filter_jobs(args: argparse.Namespace) -> tuple[list[JobConfig], dict[Any, Any]]:
    jobs = generate_jobs(args)
    log.info(f"Generated {len(jobs)} total jobs")

    log.info("Checking job statuses...")
    job_statuses = {}
    for job in jobs:
        status = get_job_status(job)
        job_statuses[id(job)] = status

    successful_count = sum(1 for s in job_statuses.values() if s == "success")
    failed_count = sum(1 for s in job_statuses.values() if s == "failed")
    not_run_count = sum(1 for s in job_statuses.values() if s == "not_run")

    log.info(
        f"Status: {successful_count} successful, {failed_count} failed, {not_run_count} not run"
    )

    if args.force_all:
        filtered_jobs = jobs
        log.info("Running all jobs (--force-all)")
    elif args.only_failed:
        filtered_jobs = [job for job in jobs if job_statuses[id(job)] == "failed"]
        log.info(f"Running only failed jobs (--only-failed): {len(filtered_jobs)} jobs")
    elif args.only_missing:
        filtered_jobs = [job for job in jobs if job_statuses[id(job)] == "not_run"]
        log.info(f"Running only un-run jobs (--only-missing): {len(filtered_jobs)} jobs")
    else:
        filtered_jobs = [job for job in jobs if job_statuses[id(job)] in ("failed", "not_run")]
        log.info(f"Running failed and un-run jobs (default): {len(filtered_jobs)} jobs")
    return filtered_jobs, job_statuses


if __name__ == "__main__":
    args = parse_args()
    main(args)