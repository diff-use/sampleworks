from typing import Any, cast

import numpy as np
import torch
from biotite.structure import AtomArray, AtomArrayStack
from jaxtyping import ArrayLike, Float, Int
from loguru import logger
from sampleworks.core.forward_models.xray.real_space_density import (
    DifferentiableTransformer,
    XMap_torch,
)
from sampleworks.core.forward_models.xray.real_space_density_deps.qfit.sf import (
    ATOM_STRUCTURE_FACTORS,
    ATOMIC_NUM_TO_ELEMENT,
    ELECTRON_SCATTERING_FACTORS,
)
from sampleworks.core.forward_models.xray.real_space_density_deps.qfit.volume import (
    XMap,
)
from sampleworks.utils.elements import normalize_element
from sampleworks.utils.torch_utils import try_gpu


def setup_scattering_params(
    atom_array: AtomArray | AtomArrayStack, em_mode: bool, device: torch.device
) -> torch.Tensor:
    """Set up atomic scattering parameters for density calculation.

    Parameters
    ----------
    atom_array
        Structure containing atoms with element information
    em_mode
        If True, use electron scattering factors (for cryo-EM).
        If False, use X-ray scattering factors.
    device
        PyTorch device to place the scattering parameter tensor on

    Returns
    -------
    torch.Tensor
        Scattering parameter tensor of shape (max_atomic_num + 1, n_coeffs, 2)
        containing scattering coefficients for each element type
    """
    elements = atom_array.element
    unique_elements = sorted(set(normalize_element(e) for e in elements))  # pyright:ignore[reportOptionalIterable]
    atomic_num_dict = {elem: ATOMIC_NUM_TO_ELEMENT.index(elem) for elem in unique_elements}

    structure_factors = ELECTRON_SCATTERING_FACTORS if em_mode else ATOM_STRUCTURE_FACTORS

    max_atomic_num = max(atomic_num_dict.values())
    n_coeffs = len(structure_factors["C"][0])
    dense_size = torch.Size([max_atomic_num + 1, n_coeffs, 2])
    scattering_tensor = torch.zeros(dense_size, dtype=torch.float32, device=device)

    for elem in unique_elements:
        atomic_num = atomic_num_dict[elem]
        if elem in structure_factors:
            factor = structure_factors[elem]
        else:
            logger.warning(f"Scattering factors for {elem} not found, using C")
            factor = structure_factors["C"]
        factor_tensor = torch.tensor(factor, dtype=torch.float32, device=device).T
        scattering_tensor[atomic_num, :, :] = factor_tensor

    return scattering_tensor


def extract_density_inputs_from_atomarray(
    atom_array: AtomArray | AtomArrayStack, device: torch.device
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """Extract and prepare atomic data for density calculation.

    Filters out atoms with invalid coordinates or zero occupancy, converts
    element names to atomic numbers, and handles NaN B-factors.

    Parameters
    ----------
    atom_array
        Structure to extract data from
    device
        PyTorch device to place tensors on

    Returns
    -------
    tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]
        Tuple of (coordinates, elements, b_factors, occupancies) as PyTorch tensors.
        All tensors have shape (1, n_atoms) or (1, n_atoms, 3) for coordinates.
    """
    coords = cast(np.ndarray[Any, np.dtype[np.float64]], atom_array.coord)
    occupancy = cast(np.ndarray[Any, np.dtype[np.float64]], atom_array.occupancy)
    b_factor = cast(np.ndarray[Any, np.dtype[np.float64]], atom_array.b_factor)
    elements = cast(np.ndarray[Any, np.dtype[np.str_]], atom_array.element)

    n_total = len(coords)
    invalid_coords_mask = ~np.isfinite(coords).all(axis=1)
    zero_occ_mask = occupancy <= 0
    valid_mask = ~invalid_coords_mask & ~zero_occ_mask

    n_invalid_coords = int(invalid_coords_mask.sum())
    n_zero_occ = int((zero_occ_mask & ~invalid_coords_mask).sum())
    n_valid = int(valid_mask.sum())

    if n_invalid_coords > 0 or n_zero_occ > 0:
        logger.info(
            f"Filtered {n_invalid_coords + n_zero_occ} atoms: "
            f"{n_invalid_coords} with invalid coords, {n_zero_occ} with zero occupancy "
            f"({n_valid}/{n_total} atoms remaining)"
        )

    coords_tensor = torch.from_numpy(coords[valid_mask]).to(device, dtype=torch.float32)
    elements_tensor = torch.tensor(
        [ATOMIC_NUM_TO_ELEMENT.index(normalize_element(e)) for e in elements[valid_mask]],
        device=device,
        dtype=torch.long,
    )
    b_factors_tensor = torch.from_numpy(b_factor[valid_mask]).to(device, dtype=torch.float32)

    nan_b_factor_mask = torch.isnan(b_factors_tensor)
    num_nan_b_factors = int(nan_b_factor_mask.sum().item())
    if num_nan_b_factors > 0:
        logger.warning(
            f"{num_nan_b_factors} atoms have NaN B-factors; assigning default B-factor of 20.0"
        )
    b_factors_tensor = torch.where(
        nan_b_factor_mask,
        torch.tensor(20.0, device=device),
        b_factors_tensor,
    )
    occupancies_tensor = torch.from_numpy(occupancy[valid_mask]).to(device, dtype=torch.float32)

    # batch dimension: (1, n_atoms, ...)
    return (
        coords_tensor.unsqueeze(0) if coords_tensor.ndim == 2 else coords_tensor,
        elements_tensor.unsqueeze(0),
        b_factors_tensor.unsqueeze(0),
        occupancies_tensor.unsqueeze(0),
    )


class RealSpaceRewardFunction:
    def __init__(
        self,
        xmap: XMap,
        scattering_params: torch.Tensor,
        selection: ArrayLike | torch.Tensor,
        em: bool = False,
        loss_order: int = 2,
        device: torch.device | None = None,
    ):
        """Hardcoded reward function for now, L1 or L2 reward for fitting real space
        electron density.

        TODO: decide whether these should take in structure or take in
        coords, bfactors, etc. separately. Leaning towards separate, as then
        the functions will be pure and we can do grad w.r.t. input."""

        if device is None:
            device = try_gpu()

        self.transformer = DifferentiableTransformer(
            xmap=XMap_torch(xmap, device=device),
            scattering_params=scattering_params,
            em=em,
            device=device,
        )

        # TODO: selection doesn't do anything right now
        self.selection = (
            selection
            if torch.is_tensor(selection)
            else torch.tensor(selection).to(device=device, dtype=torch.bool)
        )
        self.device = device
        if loss_order == 1:
            self.loss = torch.nn.L1Loss()
        elif loss_order == 2:
            self.loss = torch.nn.MSELoss()
        else:
            raise ValueError("Invalid loss_order, must be 1 or 2")

    def precompute_unique_combinations(
        self,
        elements: Int[torch.Tensor, "batch n_atoms"],
        b_factors: Float[torch.Tensor, "batch n_atoms"],
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """Pre-compute unique (element, b_factor) combinations for vmap compatibility.

        This allows torch.unique to be called outside of vmap contexts, avoiding
        the dynamic shapes.

        Parameters
        ----------
        elements: torch.Tensor
            Atomic elements, shape [batch n_atoms]
        b_factors: torch.Tensor
            Per-atom B-factors, shape [batch n_atoms]

        Returns
        -------
        tuple[torch.Tensor, torch.Tensor]
            unique_combinations: Unique (element, b_factor) pairs
            inverse_indices: Indices to reconstruct original from unique
        """
        device = self.device
        elements_flat = elements.reshape(-1)
        b_factors_flat = b_factors.reshape(-1)
        combined = torch.stack([elements_flat, b_factors_flat], dim=1)
        unique_combinations, inverse_indices = torch.unique(combined, dim=0, return_inverse=True)
        return unique_combinations.to(device), inverse_indices.to(device)

    def structure_to_reward_input(self, structure: dict) -> dict[str, Float[torch.Tensor, "..."]]:
        atom_array = structure["asym_unit"]
        atom_array = atom_array[:, atom_array.occupancy > 0]
        elements = [
            ATOMIC_NUM_TO_ELEMENT.index(normalize_element(elem)) for elem in atom_array.element
        ]

        elements = torch.tensor(elements, device=self.device).unsqueeze(0)
        b_factors = torch.from_numpy(atom_array.b_factor).to(self.device).unsqueeze(0)
        occupancies = torch.from_numpy(atom_array.occupancy).to(self.device).unsqueeze(0)

        coordinates = torch.from_numpy(atom_array.coord).to(self.device)

        return {
            "coordinates": coordinates,
            "elements": elements,
            "b_factors": b_factors,
            "occupancies": occupancies,
        }

    def __call__(
        self,
        coordinates: Float[torch.Tensor, "batch n_atoms 3"],
        elements: Float[torch.Tensor, "batch n_atoms"],
        b_factors: Float[torch.Tensor, "batch n_atoms"],
        occupancies: Float[torch.Tensor, "batch n_atoms"],
        unique_combinations: torch.Tensor | None = None,
        inverse_indices: torch.Tensor | None = None,
    ) -> Float[torch.Tensor, ""]:
        """Pure function for computing reward. Call .backward() on this to get gradients
        w.r.t. input.

        Parameters
        ----------
        coordinates: Float[torch.Tensor, "batch n_atoms 3"]
            Atomic coordinates
        elements: Float[torch.Tensor, "batch n_atoms"]
            Atomic elements
        b_factors: Float[torch.Tensor, "batch n_atoms"]
            Per-atom B-factor
        occupancies: Float[torch.Tensor, "batch n_atoms"]
            Per-atom occupancies
        unique_combinations: torch.Tensor | None, optional
            Pre-computed unique (element, b_factor) pairs for vmap compatibility
        inverse_indices: torch.Tensor | None, optional
            Pre-computed inverse indices for vmap compatibility

        Returns
        -------
        torch.Tensor
            Reward value
        """
        density: torch.Tensor = self.transformer(
            coordinates=coordinates,
            elements=elements,
            b_factors=b_factors,
            occupancies=occupancies,
            unique_combinations=unique_combinations,
            inverse_indices=inverse_indices,
        ).sum(0)  # sum over batch dimension

        return self.loss(density, self.transformer.xmap.array)
