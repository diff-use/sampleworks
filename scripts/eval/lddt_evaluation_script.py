import argparse
import itertools
import re
import sys
import traceback
from pathlib import Path

import numpy as np
import pandas as pd
from atomworks.io.transforms.atom_array import ensure_atom_array_stack
from atomworks.io.utils.io_utils import load_any
from biotite.structure import AtomArray, AtomArrayStack
from joblib import delayed, Parallel
from loguru import logger
from sampleworks.eval.eval_dataclasses import Experiment, ProteinConfig
from sampleworks.eval.grid_search_eval_utils import parse_args, scan_grid_search_results
from sampleworks.eval.structure_utils import get_reference_atomarraystack
from sampleworks.metrics.lddt import AllAtomLDDT
from sampleworks.utils.atom_array_utils import filter_to_common_atoms, map_altlocs_to_stack
from sklearn.metrics import silhouette_samples


def compute_cross_lddts(
    ref_atom_array_stack: AtomArrayStack,
    pred_atom_array_stack: AtomArrayStack,
    selection: str = "all",
) -> np.ndarray:
    """
        Compute the LDDTs between each set of coordinates in ``ref_atom_array_stack`` and each
    in ``pred_atom_array_stack``, considering only atoms that match the given selection.
    Parameters
    ----------
    ref_atom_array_stack : AtomArrayStack
        AtomArrayStack containing reference coordinates (probably an RCSB entry, where each
        structure corresponds to a separate altloc).
    pred_atom_array_stack : AtomArrayStack
        AtomArrayStack containing predicted coordinates (probably generated by a structure
        prediction model, where each structure corresponds to a separate altloc).
    selection : str, optional
        Atom selection string passed to the LDDT computation. If ``"all"``, all atoms are
        considered. Defaults to ``"all"``.

    Note
    ----
    This method does *not* select altlocs; you must do that yourself before calling it.

    Returns
    -------
    np.ndarray
        A 2D array of shape ``(len(ref_atom_array_stack), len(pred_atom_array_stack))``
        containing the LDDTs for each pair of coordinates. LDDTs are computed using
        :class:`sampleworks.metrics.lddt.AllAtomLDDT`'s ``compute`` method. The LDDT score for
        each pair is the average over the ``"residue_lddt_scores"`` output from ``compute()``.
    """
    n_ref = ref_atom_array_stack.stack_depth()
    n_pred = pred_atom_array_stack.stack_depth()

    # Initialize the result matrix
    lddt_matrix = np.zeros((n_ref, n_pred))

    # Create AllAtomLDDT metric instance
    lddt_metric = AllAtomLDDT()

    # Filter to common atoms once (they should be the same across all models in the stacks)
    ref_filtered, pred_filtered = filter_to_common_atoms(
        ref_atom_array_stack, pred_atom_array_stack
    )

    # Iterate over all pairs of reference and predicted structures
    for i in range(n_ref):
        for j in range(n_pred):
            # Extract individual AtomArrays for this pair
            ref_single: AtomArray = ref_filtered[i]
            pred_single: AtomArray = pred_filtered[j]

            # Compute LDDT using AllAtomLDDT
            # Pass selection parameter if not "all"
            if selection == "all":
                result = lddt_metric.compute(pred_single, ref_single, selection=None)
            else:
                result = lddt_metric.compute(pred_single, ref_single, selection=selection)

            # Extract residue-level LDDT scores
            residue_lddt_scores = result["residue_lddt_scores"]

            # Compute average over all residue-level LDDT scores
            # Each residue has a list of scores (one per model, but we compute on single models)
            all_scores = []
            for residue_scores_list in residue_lddt_scores.values():
                # residue_scores_list is a list, typ. with one value for single model comparison
                all_scores.extend(residue_scores_list)

            # Calculate the mean LDDT score
            if all_scores:
                lddt_matrix[i, j] = np.mean(all_scores)
            else:
                # If no scores computed (e.g., no residues match selection), set to NaN
                lddt_matrix[i, j] = np.nan

    return lddt_matrix


def nn_lddt_clustering(
    ref_atom_array_stack: AtomArrayStack,
    pred_atom_array_stack: AtomArrayStack,
    selection: str = "all",
) -> dict[str, float | np.ndarray | list[float]]:
    """
    ref_atom_array_stack: AtomArrayStack containing reference coordinates (probably an RCSB entry,
        where each structure corresponds to a separate altloc)

    pred_atom_array_stack: AtomArrayStack containing predicted coordinates (probably generated by
        a structure prediction model, where each structure corresponds to a separate altloc)

    NOTE: this method DOES NOT select altlocs, you must do that yourself.

    This method assigns a mapping from each predicted structure to the closest reference structure,
    based on the LDDT score. It then computes a silhouette score assuming that mapping as a
    clustering.

    It returns the average silhouette score for the clustering, a sort-of silhouette score assuming
    the cluster centers are the true (reference) structures, and a list of the occupancies of the
    clusters.

    Returns:
        A dictionary:
        {"avg_silhouette": float, "avg_silhouette_to_ref": float, "occupancies": list[float]}
    """
    # check that the reference array has at least two structures--otherwise this makes no sense.
    if ref_atom_array_stack.stack_depth() < 2:
        raise ValueError("Reference AtomArrayStack must contain at least two structures")

    # First compute the self-LDDT matrix between all structures in the predicted stack
    self_lddt_matrix = compute_cross_lddts(pred_atom_array_stack, pred_atom_array_stack, selection)

    # Second, compute the cross-LDDT matrix between all structures in the predicted stack and
    # all structures in the reference stack
    cross_lddt_matrix = compute_cross_lddts(ref_atom_array_stack, pred_atom_array_stack, selection)

    # Assign each predicted structure to the closest reference structure based on LDDT score (i.e.,
    # the reference structure with the highest LDDT score is assigned to the predicted structure)
    closest_ref_indices = np.argmax(cross_lddt_matrix, axis=0)

    # Compute the silhouette score assuming the nearest-reference neighbor clustering.
    if len(np.unique(closest_ref_indices)) < 2:
        sscore = np.nan  # All points are assigned to the same cluster, the score doesn't make sense
    else:
        lddt_distance = 1 - self_lddt_matrix
        ssamples = silhouette_samples(lddt_distance, closest_ref_indices, metric="precomputed")
        sscore = float(np.mean(ssamples))

    # Compute the occupancy of each cluster based on the number of structures in the cluster
    cluster_sizes = np.bincount(closest_ref_indices, minlength=len(ref_atom_array_stack))
    occupancy_levels = cluster_sizes / cluster_sizes.sum()

    # compute something like a silhouette score, comparing each point's distance to it's assigned
    # cluster center (the reference structure) to its next-nearest reference structure.
    #  First, partition the "lddt distance" matrix; this ensures the top two values are the
    #  smallest and second-smallest distances, respectively.
    partitioned_distances = np.partition(1 - cross_lddt_matrix, 1, axis=0)

    # for each cluster, compute something like a silhouette score
    avg_silhouette_to_ref = np.mean(1 - partitioned_distances[0] / partitioned_distances[1])

    return {
        "avg_silhouette": sscore,
        "occupancies": occupancy_levels,
        "avg_silhouette_to_ref": avg_silhouette_to_ref,
        "self_lddt_matrix": self_lddt_matrix,
        "cross_lddt_matrix": cross_lddt_matrix,
        "closest_ref_indices": closest_ref_indices,
    }


def translate_selection(selection: str) -> str:
    # current selection strings are pymol like, and we want to convert to atomworks/pandas like
    # this should be a temporary measure only until we switch to atomworks style in the RSCC script

    if any(x in selection for x in ("==", ">", "<", "<=", ">=", " in ")):
        # assume this is already atomworks/pandas style and ignore.
        return selection

    DeprecationWarning(
        "DEPRECATED: translate_selection converts from some pymol-like selection strings to "
        "AtomWorks selection strings, but is not guaranteed to be correct for all cases."
    )

    pattern = re.compile(r"chain ([A-Z]) and resi (\d+)-(\d+)")
    match = pattern.search(selection)
    if match is None:
        raise RuntimeError(f"Failed to match selection string {selection}")
    new_selection = f"chain_id == '{match.group(1)}' "
    new_selection += f"and res_id >= {match.group(2)} and res_id <= {match.group(3)}"
    return new_selection


def main(args: argparse.Namespace):
    workspace_root = Path(args.workspace_root)

    # TODO make more general: https://github.com/diff-use/sampleworks/issues/93
    grid_search_dir = workspace_root / "grid_search_results"

    # Protein configurations: base map paths, structure selections, and resolutions
    protein_inputs_dir = args.grid_search_inputs_path or workspace_root
    protein_configs = ProteinConfig.from_csv(protein_inputs_dir, args.protein_configs_csv)

    logger.info(f"Grid search directory: {grid_search_dir}")
    logger.info(f"Proteins configured: {list(protein_configs.keys())}")

    # Scan for experiments (look for refined.cif files)
    all_experiments = scan_grid_search_results(grid_search_dir)
    logger.info(f"Found {len(all_experiments)} experiments with refined.cif files")

    if all_experiments:
        all_experiments.summarize()  # Prints some summary stats, e.g. number of unique proteins
    else:
        logger.error("No experiments found in grid search directory. Exiting with status 1.")
        sys.exit(1)

    logger.info("Pre-loading reference structures for each protein for coordinate extraction")
    reference_atom_arrays = {}
    for protein_key, protein_config in protein_configs.items():
        for occ, sel in itertools.product(args.occupancies, protein_config.selection):
            ref_path, reference_proteins = get_reference_atomarraystack(protein_config, occ)
            if reference_proteins is None:
                logger.warning(
                    f"Could not find ref structure for {protein_key} and occupancy {occ}"
                )
                continue
            try:
                logger.info(
                    f"Loaded ref structure for {protein_key} and occupancy {occ}: {ref_path}"
                )
                reference_protein_stack, _, _ = map_altlocs_to_stack(
                    reference_proteins, selection=translate_selection(sel), return_full_array=True
                )
                # hierarchical dictionary cache makes it lighter weight to parallelize.
                if (protein_key, occ) not in reference_atom_arrays:
                    reference_atom_arrays[(protein_key, occ)] = {}

                reference_atom_arrays[(protein_key, occ)][sel] = reference_protein_stack
            except Exception as e:
                logger.error(
                    f"Error loading ref structure for {protein_key} and occupancy {occ}: {e}"
                )
                logger.error(f"  Traceback: {traceback.format_exc()}")

    # Do the quick pass through all the "rows" of our output table to filter in those we can run.
    filtered_experiments = []
    null_results = []
    for _exp in all_experiments:
        if _exp.protein in protein_configs:
            protein = _exp.protein
        elif _exp.protein.upper() in protein_configs:
            protein = _exp.protein.upper()
        elif _exp.protein.lower() in protein_configs:
            protein = _exp.protein.lower()
        else:
            # These we just skip over--we assume that the user has told us via the config file
            # what results they are interested in.
            logger.warning(f"Skipping protein with no configuration: {_exp.protein}")
            continue

        protein_config = protein_configs[protein]
        if protein_config.protein != protein:
            raise ValueError(
                f"Protein name mismatch: expected {protein_config.protein}, got {protein}, make"
                f"sure you loaded your protein configs with ProteinConfig.from_csv()."
            )

        if (protein, _exp.occ_a) not in reference_atom_arrays:
            logger.warning(
                f"Skipping {_exp.protein_dir_name}: no reference atom array stack available "
                f"for {_exp.protein}, occupancy {_exp.occ_a}."
            )
            # record empty results for all selections, indicating they could not be computed.
            for _sel in protein_config.selection:
                exp_copy = _exp.__dict__.copy()
                exp_copy["selection"] = _sel
                null_results.append(exp_copy)
            continue

        protein_reference_atom_arrays = reference_atom_arrays[(protein, _exp.occ_a)]
        for _sel in protein_config.selection:
            if _sel not in protein_reference_atom_arrays:
                logger.warning(
                    f"Skipping {_exp.protein_dir_name}: no reference atom array stack available "
                    f"for {_exp.protein}, occupancy {_exp.occ_a} and selection '{_sel}'."
                )
                exp_copy = _exp.__dict__.copy()
                exp_copy["selection"] = _sel
                null_results.append(exp_copy)
                continue

            px_seln_refernce_atom_array = protein_reference_atom_arrays[_sel]
            filtered_experiments.append((_exp, protein_config, px_seln_refernce_atom_array, _sel))

    # now we can more easily parallelize this loop.
    logger.debug("Starting LDDT evaluation loop. This may take a while...")
    all_results = Parallel(n_jobs=args.n_jobs)(
        delayed(process_exp_with_selection)(
            _exp, protein_config, px_seln_refernce_atom_array, selection
        )
        for _exp, protein_config, px_seln_refernce_atom_array, selection in filtered_experiments
    )

    df = pd.DataFrame(null_results + all_results)
    df.to_csv(grid_search_dir / "lddt_results.csv", index=False)


def process_exp_with_selection(
    exp: Experiment,
    protein_config: ProteinConfig,
    px_seln_refernce_atom_array: AtomArrayStack,
    selection_string: str,
) -> dict[str, str | float | list[float]]:
    """

    Parameters
    ----------
    exp: Experiment, a description of the structure generation experiment
    protein_config: ProteinConfig, specifying the locations of reference structures and maps
    px_seln_refernce_atom_array: AtomArrayStack,
        the atom array stack for the reference structure, which in principle could be fetched
        using the protein_config, but for efficiency we load once previously and pass in here,
        since this method will run many times in parallel using the same structure
    selection_string: str, the selection string for the evaluation

    Returns
    -------
        A dictionary of results of LDDT-based clustering that can be collated in a dataframe.
        In addition to the data in the `exp` object, this dictionary contains:
        - occupancies: list[float], the occupancies of the selected atoms, computed as the
            fraction of structures in the experiment that are closest to one or the other
            altloc of the reference structure. .
        - avg_silhouette: float, the average silhouette score for the LDDT-based clustering
        - avg_silhouette_to_ref: float,
            a sort of silhouette score, measuring each structure's relative "closeness" to the
            assigned reference altloc.
    """
    logger.debug(f"Evaluating selection {selection_string} for protein {protein_config}")
    result = exp.__dict__.copy()
    result["selection"] = selection_string

    try:
        # generated structures shouldn't have altlocs, don't need altloc="all".
        predicted_atom_array_stack = load_any(exp.refined_cif_path)
        clustering_results = nn_lddt_clustering(
            px_seln_refernce_atom_array,
            ensure_atom_array_stack(predicted_atom_array_stack),
            translate_selection(selection_string),
        )

        lddt_result_keys = ("occupancies", "avg_silhouette", "avg_silhouette_to_ref")
        result.update({k: clustering_results[k] for k in lddt_result_keys})

        logger.info(
            f"Successfully processed {exp.protein_dir_name} w/ selection {selection_string}"
        )

    except Exception as e:
        logger.error(f"Error processing experiment {exp.exp_dir}: {e}")
        logger.error(f"  Traceback: {traceback.format_exc()}")
        result["error"] = str(e)
        result["avg_silhouette"] = np.nan
        result["avg_silhouette_to_ref"] = np.nan
        result["occupancies"] = []

    return result


if __name__ == "__main__":
    args = parse_args("Evaluate LDDT on grid search results.")
    main(args)
