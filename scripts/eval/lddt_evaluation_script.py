import argparse
import itertools
import re
import sys
import traceback
from pathlib import Path

import numpy as np
import pandas as pd
from atomworks.io.transforms.atom_array import ensure_atom_array_stack
from atomworks.io.utils.io_utils import load_any
from biotite.structure import AtomArray, AtomArrayStack
from loguru import logger
from sampleworks.eval.eval_dataclasses import ProteinConfig
from sampleworks.eval.grid_search_eval_utils import parse_args, scan_grid_search_results
from sampleworks.eval.structure_utils import get_reference_atomarraystack
from sampleworks.metrics.lddt import AllAtomLDDT
from sampleworks.utils.atom_array_utils import filter_to_common_atoms, map_altlocs_to_stack
from sklearn.metrics import silhouette_samples


def compute_cross_lddts(
    ref_atom_array_stack: AtomArrayStack,
    pred_atom_array_stack: AtomArrayStack,
    selection: str = "all",
) -> np.ndarray:
    """
        Compute the LDDTs between each set of coordinates in ``ref_atom_array_stack`` and each
    in ``pred_atom_array_stack``, considering only atoms that match the given selection.
    Parameters
    ----------
    ref_atom_array_stack : AtomArrayStack
        AtomArrayStack containing reference coordinates (probably an RCSB entry, where each
        structure corresponds to a separate altloc).
    pred_atom_array_stack : AtomArrayStack
        AtomArrayStack containing predicted coordinates (probably generated by a structure
        prediction model, where each structure corresponds to a separate altloc).
    selection : str, optional
        Atom selection string passed to the LDDT computation. If ``"all"``, all atoms are
        considered. Defaults to ``"all"``.

    Note
    ----
    This method does *not* select altlocs; you must do that yourself before calling it.

    Returns
    -------
    np.ndarray
        A 2D array of shape ``(len(ref_atom_array_stack), len(pred_atom_array_stack))``
        containing the LDDTs for each pair of coordinates. LDDTs are computed using
        :class:`sampleworks.metrics.lddt.AllAtomLDDT`'s ``compute`` method. The LDDT score for
        each pair is the average over the ``"residue_lddt_scores"`` output from ``compute()``.
    """
    n_ref = ref_atom_array_stack.stack_depth()
    n_pred = pred_atom_array_stack.stack_depth()

    # Initialize the result matrix
    lddt_matrix = np.zeros((n_ref, n_pred))

    # Create AllAtomLDDT metric instance
    lddt_metric = AllAtomLDDT()

    # Filter to common atoms once (they should be the same across all models in the stacks)
    ref_filtered, pred_filtered = filter_to_common_atoms(
        ref_atom_array_stack, pred_atom_array_stack
    )

    # Iterate over all pairs of reference and predicted structures
    for i in range(n_ref):
        for j in range(n_pred):
            # Extract individual AtomArrays for this pair
            ref_single: AtomArray = ref_filtered[i]
            pred_single: AtomArray = pred_filtered[j]

            # Compute LDDT using AllAtomLDDT
            # Pass selection parameter if not "all"
            if selection == "all":
                result = lddt_metric.compute(pred_single, ref_single, selection=None)
            else:
                result = lddt_metric.compute(pred_single, ref_single, selection=selection)

            # Extract residue-level LDDT scores
            residue_lddt_scores = result["residue_lddt_scores"]

            # Compute average over all residue-level LDDT scores
            # Each residue has a list of scores (one per model, but we compute on single models)
            all_scores = []
            for residue_scores_list in residue_lddt_scores.values():
                # residue_scores_list is a list, typ. with one value for single model comparison
                all_scores.extend(residue_scores_list)

            # Calculate the mean LDDT score
            if all_scores:
                lddt_matrix[i, j] = np.mean(all_scores)
            else:
                # If no scores computed (e.g., no residues match selection), set to NaN
                lddt_matrix[i, j] = np.nan

    return lddt_matrix


def nn_lddt_clustering(
    ref_atom_array_stack: AtomArrayStack,
    pred_atom_array_stack: AtomArrayStack,
    selection: str = "all",
) -> dict[str, float | np.ndarray | list[float]]:
    """
    ref_atom_array_stack: AtomArrayStack containing reference coordinates (probably an RCSB entry,
        where each structure corresponds to a separate altloc)

    pred_atom_array_stack: AtomArrayStack containing predicted coordinates (probably generated by
        a structure prediction model, where each structure corresponds to a separate altloc)

    NOTE: this method DOES NOT select altlocs, you must do that yourself.

    This method assigns a mapping from each predicted structure to the closest reference structure,
    based on the LDDT score. It then computes a silhouette score assuming that mapping as a
    clustering.

    It returns the average silhouette score for the clustering, a sort-of silhouette score assuming
    the cluster centers are the true (reference) structures, and a list of the occupancies of the
    clusters.

    Returns:
        A dictionary:
        {"avg_silhouette": float, "avg_silhouette_to_ref": float, "occupancies": list[float]}
    """
    # check that the reference array has at least two structures--otherwise this makes no sense.
    if ref_atom_array_stack.stack_depth() < 2:
        raise ValueError("Reference AtomArrayStack must contain at least two structures")

    # First compute the self-LDDT matrix between all structures in the predicted stack
    self_lddt_matrix = compute_cross_lddts(pred_atom_array_stack, pred_atom_array_stack, selection)

    # Second, compute the cross-LDDT matrix between all structures in the predicted stack and
    # all structures in the reference stack
    cross_lddt_matrix = compute_cross_lddts(ref_atom_array_stack, pred_atom_array_stack, selection)

    # Assign each predicted structure to the closest reference structure based on LDDT score (i.e.,
    # the reference structure with the highest LDDT score is assigned to the predicted structure)
    closest_ref_indices = np.argmax(cross_lddt_matrix, axis=0)

    # Compute the silhouette score assuming the nearest-reference neighbor clustering.
    if len(np.unique(closest_ref_indices)) < 2:
        sscore = np.nan  # All points are assigned to the same cluster, the score doesn't make sense
    else:
        lddt_distance = 1 - self_lddt_matrix
        ssamples = silhouette_samples(lddt_distance, closest_ref_indices, metric="precomputed")
        # ssamples should be array-like, pyright claims there's no defn of np.mean for that? 
        sscore = float(np.mean(ssamples))  # pyright:ignore

    # Compute the occupancy of each cluster based on the number of structures in the cluster
    cluster_sizes = np.bincount(closest_ref_indices, minlength=len(ref_atom_array_stack))
    occupancy_levels = cluster_sizes / cluster_sizes.sum()

    # compute something like a silhouette score, comparing each point's distance to it's assigned
    # cluster center (the reference structure) to its next-nearest reference structure.
    #  First, partition the "lddt distance" matrix; this ensures the top two values are the
    #  smallest and second-smallest distances, respectively.
    partitioned_distances = np.partition(1 - cross_lddt_matrix, 1, axis=0)

    # for each cluster, compute something like a silhouette score
    avg_silhouette_to_ref = np.mean(1 - partitioned_distances[0] / partitioned_distances[1])

    return {
        "avg_silhouette": sscore,
        "occupancies": occupancy_levels,
        "avg_silhouette_to_ref": avg_silhouette_to_ref,
        "self_lddt_matrix": self_lddt_matrix,
        "cross_lddt_matrix": cross_lddt_matrix,
        "closest_ref_indices": closest_ref_indices,
    }


def translate_selection(selection: str) -> str:
    # current selection strings are pymol like, and we want to convert to atomworks/pandas like
    # this should be a temporary measure only until we switch to atomworks style in the RSCC script

    logger.warning(
        "DEPRECATED: translate_selection converts from some pymol-like selection strings to "
        "AtomWorks selection strings, but is not guaranteed to be correct for all cases."
    )

    pattern = re.compile(r"chain ([A-Z]) and resi (\d+)-(\d+)")
    match = pattern.search(selection)
    if match is None:
        raise RuntimeError(f"Failed to match selection string {selection}")
    new_selection = f"chain_id == '{match.group(1)}' "
    new_selection += f"and res_id >= {match.group(2)} and res_id <= {match.group(3)}"
    return new_selection


def main(args: argparse.Namespace):
    workspace_root = Path(args.workspace_root)

    # TODO make more general: https://github.com/diff-use/sampleworks/issues/93
    grid_search_dir = workspace_root / "grid_search_results"

    # Protein configurations: base map paths, structure selections, and resolutions
    protein_inputs_dir = args.grid_search_inputs_path or workspace_root
    protein_configs = ProteinConfig.from_csv(protein_inputs_dir, args.protein_configs_csv)

    logger.info(f"Grid search directory: {grid_search_dir}")
    logger.info(f"Proteins configured: {list(protein_configs.keys())}")

    # Scan for experiments (look for refined.cif files)
    all_experiments = scan_grid_search_results(grid_search_dir)
    logger.info(f"Found {len(all_experiments)} experiments with refined.cif files")

    if all_experiments:
        all_experiments.summarize()  # Prints some summary stats, e.g. number of unique proteins
    else:
        logger.error("No experiments found in grid search directory. Exiting with status 1.")
        sys.exit(1)

    logger.info("Pre-loading reference structures for each protein for coordinate extraction")
    reference_atom_arrays = {}
    for protein_key, protein_config in protein_configs.items():
        for occ, sel in itertools.product(args.occupancies, protein_config.selection):
            ref_path, reference_proteins = get_reference_atomarraystack(protein_config, occ)
            if reference_proteins is None:
                logger.warning(
                    f"Could not find ref structure for {protein_key} and occupancy {occ}"
                )
                continue
            try:
                logger.info(
                    f"Loaded ref structure for {protein_key} and occupancy {occ}: {ref_path}"
                )
                reference_protein_stack, _, _ = map_altlocs_to_stack(
                    reference_proteins, selection=translate_selection(sel), return_full_array=True
                )
                reference_atom_arrays[(protein_key, occ, sel)] = reference_protein_stack
            except Exception as e:
                logger.error(
                    f"Error loading ref structure for {protein_key} and occupancy {occ}: {e}"
                )
                logger.error(f"  Traceback: {traceback.format_exc()}")

    all_results = []
    # TODO parallelize this loop? It will require replicating `reference_atom_arrays`
    #  https://github.com/diff-use/sampleworks/issues/98
    for _i, _exp in enumerate(all_experiments):
        if _exp.protein in protein_configs:
            protein = _exp.protein
        elif _exp.protein.upper() in protein_configs:
            protein = _exp.protein.upper()
        else:
            logger.warning(f"Skipping protein with no configuration: {_exp.protein}")
            continue

        protein_config = protein_configs[protein]

        for selection in protein_config.selection:
            result = _exp.__dict__.copy()
            result["selection"] = selection
            atom_array_key = (protein, _exp.occ_a, selection)
            if atom_array_key not in reference_atom_arrays:
                logger.warning(
                    f"Skipping {_exp.protein_dir_name}: no reference atom array stack available "
                    f"for {_exp.protein}, occupancy {_exp.occ_a} and selection '{selection}'."
                )
                continue

            try:
                reference_atom_array_stack = reference_atom_arrays[atom_array_key]
                # generated structures shouldn't have altlocs, don't need altloc="all".
                predicted_atom_array_stack = load_any(_exp.refined_cif_path)
                clustering_results = nn_lddt_clustering(
                    reference_atom_array_stack,
                    ensure_atom_array_stack(predicted_atom_array_stack),
                    translate_selection(selection),
                )

                result.update(
                    {
                        k: clustering_results[k]
                        for k in ("occupancies", "avg_silhouette", "avg_silhouette_to_ref")
                    }
                )
            except Exception as e:
                logger.error(f"Error processing experiment {_exp.exp_dir}: {e}")
                logger.error(f"  Traceback: {traceback.format_exc()}")
                result["error"] = str(e)
                result["avg_silhouette"] = np.nan
                result["avg_silhouette_to_ref"] = np.nan
                result["occupancies"] = []

            all_results.append(result)

    df = pd.DataFrame(all_results)
    df.to_csv(grid_search_dir / "lddt_results.csv", index=False)


if __name__ == "__main__":
    args = parse_args("Evaluate LDDT on grid search results.")
    main(args)
